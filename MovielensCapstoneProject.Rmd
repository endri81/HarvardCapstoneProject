---
title: "Capstone Project Report"
subtitle: "Building a recommender system using 10M version of Movielens dataset"
author: "Endri Raco"
output:
  pdf_document:
    df_print: kable
    toc: yes
documentclass: article
classoption: a4paper
fig_height: 5
fig_width: 5
fontsize: 10pt
highlight: zenburn
latex_engine: xelatex
mainfont: Arial
mathfont: LiberationMono
monofont: DejaVu Sans Mono
---

  
```{r setup, include=FALSE, echo=FALSE,warning=FALSE}
library(knitr)
knitr::opts_chunk$set(
  verbose=TRUE,
  root.dir=normalizePath('../'),
  fig.path ='../figures/',
  comment = NA,
  warning=FALSE,
  message=FALSE,
  fig.align='center',
  fig.lp = '',
  fig.keep='high',
  fig.show='hold',
  echo=TRUE, 
  tidy.opts=list(width.cutoff=60),
  tidy = FALSE, 
  dev='pdf')
```

```{r wrap-hook, echo=FALSE}
# Function to make output fit on page
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```
## Dedication

This project and all my work is dedicated to victims of Albanian earthquake 26 November 2019.


## Acknowledgement

I would like to express my special thanks of gratitude to Prof. Rafael Irizarry for the wonderful material and thorough explanations he provided during all courses. Also I want to thank my friends of this course who share the same interests for Data Science. 

## Introduction

&nbsp;

Recommender systems are information filtering tools that aspire to predict the rating for users and items, predominantly from big data to recommend their likes. Movie recommendation systems provide a mechanism to assist users in classifying users with similar interests. This makes recommender systems essentially a central part of websites and e-commerce applications. This project focuses on building a movie recommendation system using data from 10M version of movielens dataset. Several Machine Learning techniques such as Matrix Factorization, Regularization etc will be used to produce evaluation metrics such as root mean square error (RMSE) for the movie recommender system. For all project calculations is used the following PC:


```{r pc}
print("Operating System:")
version
```

&nbsp;

## Importing data

&nbsp;

MovieLense dataset contains the ratings that the users give to movies. Code used in 'Importing data' section was previously provided by edX. 

Let's start by checking if needed R packages for this project are installed. If not, code below will install them. 

&nbsp;

```{r required_packages}
# required packages for our project
if(!require(kableExtra)) install.packages('kableExtra', 
repos = 'http://cran.us.r-project.org')
if(!require(dataCompareR)) install.packages('dataCompareR', 
repos = 'http://cran.us.r-project.org')
if(!require(tidyverse)) install.packages('tidyverse', 
repos = 'http://cran.us.r-project.org')
if(!require(caret)) install.packages('caret', 
repos = 'http://cran.us.r-project.org')
if(!require(data.table)) install.packages('data.table', 
repos = 'http://cran.us.r-project.org')
```

&nbsp;

Now we are ready for data downloading:

&nbsp;

```{r data_download, eval=FALSE, cache=TRUE}
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file('http://files.grouplens.org/datasets/movielens/ml-10m.zip', dl)
```

&nbsp;

Some adjustments of downloaded data to have **movielens** dataframe as a result

&nbsp;

```{r arrange_data, eval=FALSE, cache=TRUE}
ratings <- fread(text = gsub('::', '\t', 
readLines(unzip(dl, 'ml-10M100K/ratings.dat'))),
col.names = c('userId', 'movieId', 'rating', 'timestamp'))
movies <- str_split_fixed(readLines(unzip(dl, 'ml-10M100K/movies.dat')), '\\::', 3)
colnames(movies) <- c('movieId', 'title', 'genres')
movies <- as.data.frame(movies) %>% 
mutate(movieId = as.numeric(levels(movieId))[movieId],
title = as.character(title),
genres = as.character(genres))
movielens <- left_join(ratings, movies, by = 'movieId')
```

&nbsp;

When developing an algorithm, we usually have a dataset for which we know the outcomes, as we do with the heights: we know the sex of every student in our dataset. Therefore, to mimic
the ultimate evaluation process, we typically split the data into two parts and act as if we don’t know the outcome for one of these. We stop pretending we don’t know the outcome to evaluate the algorithm, but only after we are done constructing it. We refer to the group for which we know the outcome, and use to develop the algorithm, as the training set. We refer to the group for which we pretend we don’t know the outcome as the test set. A standard way of generating the training and test sets is by randomly splitting the data. The caret package includes the function **createDataPartition** that helps us generates indexes for randomly splitting the data into training and test sets:

&nbsp;

```{r data-partititon, eval = FALSE, cache=TRUE}
# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind='Rounding')
test_index <- createDataPartition(y = movielens$rating, 
times = 1, p = 0.1, list = FALSE)
```

&nbsp;

We use the result of the **createDataPartition** function call to define the training and test sets like this:

&nbsp;

```{r data-partititon2, eval = FALSE,cache=TRUE}
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
```

&nbsp;

And some final adjustments before cleaning environment from unused elements.

&nbsp;

```{r final-adjustments, eval = FALSE, cache=TRUE}
# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = 'movieId') %>%
  semi_join(edx, by = 'userId')

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

&nbsp;

Now, we can save the result of steps above (**edx** and **validation** dataframes ) as R objects, so we can reload the final version of the data into the session for further analysis without repeating the process.

&nbsp;

```{r save-data, eval = FALSE, cache=TRUE}
# Save our data as R objects
save(edx, file = 'edx.RData')
save(validation, file = 'validation.RData')
```

&nbsp;

## Describing Data

&nbsp;

We stored the data for our project in two data frames. Let's access these datasets using the **load** function:
  
&nbsp;

```{r load-data, eval=TRUE}
# Load data
load('edx.RData')
load('validation.RData')
```

&nbsp;

First, we make a check if our data format is indeed **data frame**:
  
  &nbsp;

```{r data-format, eval=TRUE}
# Check format
class(edx)
class(validation)
```

&nbsp;

Now let's take a look in our data. We start by finding out more about the structure of our **edx**:

&nbsp;

```{r data-str-edx,eval=TRUE, linewidth=60, cache=TRUE}
as_tibble(edx) %>%
slice(1:5) %>%
knitr::kable()
```


&nbsp;

Now for **validation**:

```{r data-str-val,eval=TRUE, linewidth=60, cache=TRUE}
as_tibble(validation) %>%
slice(1:5) %>%
knitr::kable()
```

&nbsp;

We see that **edx** data frame has `r nrow(edx)` rows and `r ncol(edx)`
variables, while **validation** data frame has `r nrow(validation)` rows and `r ncol(validation)`.

Now let's print features of both data frames **edx** and **validation together** to reassure ourselves that both contain the same features.

&nbsp;

```{r basic_info, eval=TRUE,linewidth=60, cache=TRUE}
library(dataCompareR)
comp_edx_val <- rCompare(edx, validation)
comp_summ <- summary(comp_edx_val)
comp_summ[c('datasetSummary', 'ncolInAOnly', 'ncolInBOnly', 'ncolCommon', 'rowsInAOnly', 'rowsInBOnly', 'nrowCommon')] 
```

&nbsp;

It is a good idea to check for dublicates so to create a general idea about number of distinct users, movies and genres.

&nbsp;

```{r distinct_data, eval=TRUE,cache=TRUE}
# Distinct users, movies, genres
dist_col <- edx %>% 
summarize(distinct_users = n_distinct(userId),
            distinct_movies = n_distinct(movieId),
            distinct_genres = n_distinct(genres))
knitr::kable(dist_col)
```

&nbsp;


## Data Wrangling

When we printed **edx** and **validation** data frames as tibbles we noticed that we can make some arrangements in **title**, **timestamp** and **genres** columns to bring our data in a tidy format.

We are going to perform these tasks:
  
  - Most of the movies have their **premier year** added to their **titles**. We will extract debut years in a separate column.

  - Column **genres** has to be categorized. We will change the class of **genres** to **factor**
  
  - **Timestamp** needs to be converted to **rate_year**.

For the sake of analysis we will need **userId** and **movieId** converted from class **integer** to class **factor**.   

&nbsp;

```{r tidy-data, eval = TRUE,cache=TRUE}
tidydf <- function(df){
  df$genres <- as.factor(df$genres) #Convert genres to factor
  df$timestamp <- as.Date(as.POSIXct(df$timestamp, origin='1970-01-01'))
  #Convert timestamp
  names(df)[names(df) == 'timestamp'] <- 'rate_year' # Rename column timestamp to rate_year
  df <- df %>% 
    mutate(title = str_trim(title), rate_year = year(rate_year)) %>%  #Mutate title and rate_year
    extract(title, c('title', 'premier_year'), regex = '(.*)\\s\\((\\d+)\\)', convert = TRUE) 
#Separate title from year
return(df)
}
# Transform our dataframes
edx <- tidydf(edx)
validation <- tidydf(validation)
```


Now our data frames look like this:
  
&nbsp;

```{r check-tidy-data, eval=TRUE,cache=TRUE}
as_tibble(edx)
as_tibble(validation)
```

&nbsp;

Probably is a good idea in this step to check for NA values:
  
&nbsp;

```{r check-na-data, eval=TRUE,cache=TRUE}
# Check edx dataframe for NA values
edx_na <- edx %>%
filter(is.na(title) | is.na(year))
glimpse(edx_na) 

# Check validation dataframe for NA values
validation_na <- validation %>%
filter(is.na(title) | is.na(year))
glimpse(validation_na) 
```

&nbsp;


## Exploring Data

&nbsp;

### Ratings frequency

&nbsp;

From this step to the development of our algorithm we will continue using **edx** dataframe. We will come back to **validation** dataframe to perform  a final test of our algorithm, predict movie ratings in the validation set as if they were unknown. 

&nbsp;

Now let's begin our exploration by looking at **rating** variable.


```{r rate-explore, eval=TRUE,cache=TRUE}
# Check frequencies of ratings unique values
table_rating <- as.data.frame(table(edx$rating))
colnames(table_rating) <- c('Rating', 'Frequencies')
knitr::kable(table_rating)
```





Now, we will build a frequency plot of the ratings using **ggplot2**. For this step we need to convert **ratings** into categories:

&nbsp;

```{r rating-frequency-plot, eval=TRUE,cache=TRUE}
# Frequency plot of the ratings
table_rating %>% ggplot(aes(Rating, Frequencies)) +
geom_bar(stat = 'identity') +
labs(x='Ratings', y='Count') +
ggtitle('Distribution of ratings')
```

&nbsp;

We notice from the figure that most of the ratings are above 2, and the most common is 4. 

&nbsp;

### Most viewed movies

Now let's check 10 most viewed movies:
  
&nbsp;

```{r movie-view, eval=TRUE,cache=TRUE}
# Top movies by number of views
tmovies <- edx %>% select(title) %>% 
group_by(title) %>% 
summarize(count=n()) %>% 
arrange(desc(count)) 
# Print top_movies
knitr::kable(head(tmovies,10))
```

&nbsp;

As we can see from the table, the most viewed movie is **Pulp Fiction** with 
`r print(head(tmovies,10)[[1,2]])` ratings.


### Ratings average

Our next task is to identify the top-rated movies by computing the **average score** for each of
them. 

&nbsp;

```{r top-rated, eval=TRUE,cache=TRUE}
# Top movies by rating average
rating_avg <- edx %>%
  select(title, rating) %>%
  group_by(title) %>%
  summarise(count = n(), avg = mean(rating), min = min(rating), max = max(rating)) %>%
  arrange(desc(avg))
# Print top_movies
knitr::kable(head(rating_avg,10))
```

&nbsp;

By looking at the table above we get the feel that something is wrong. We see that best average rating is for movies with a very low number of ratings (sometimes only one person has voted for this movie). Since this doesn't help getting our analysis right, we have to treat these records as outliers and exclude them from further analysis. We will remove the movies whose number of views is below a defined threshold, for instance, below 200:

&nbsp;

```{r top-rated-200, eval=TRUE,cache=TRUE}
# Top movies by rating average
rating_avg_200 <- edx %>%
select(title, rating) %>%
group_by(title) %>%
summarise(count = n(), avg = mean(rating), min = min(rating), max = max(rating)) %>%
filter(count > 200) %>%  
arrange(desc(avg))
# Print top_movies
knitr::kable(head(rating_avg_200,10))
```

&nbsp;

Let's built the chart:
  
&nbsp;

```{r top-rated-chart, eval=TRUE,cache=TRUE}
rating_avg_200 %>% 
  ggplot(aes(x= avg, fill = count)) +
  geom_histogram( binwidth = 0.2) +
  scale_x_continuous(breaks=seq(0, 5, by= 0.5)) +
  labs(x='rating average', y='count') +
  ggtitle('Distribution of average ratings ') 
```

&nbsp;

Largest number of rankings are between 3 and 4. The highest value it is around 4. It is interesting that no user has rated movies with 0s.

### Data as matrix

As professor Rafael Irizarry mentioned in his book, we can think of our dataframe **edx** as a
very large matrix, with users on the rows and movies on the columns. To see how sparse the matrix is, we have to convert **edx** to this format. The figure below shows the matrix for a random sample of 100 movies and 100 users with blue indicating a user/movie combination for which we have a rating

&nbsp;

```{r edx-matrix, eval=TRUE,cache=TRUE}
# We create a copy of existing edx
edx_copy <-edx
# Sample of 100 users 
users <- sample(unique(edx_copy$userId), 100)
edx_copy %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,., col = 'blue', xlab='Movies', ylab='Users', main = 'Heatmap of the movie rates matrix')

```

&nbsp;

However, this chart is just displaying some random users and items. What if, instead, want to visualize only the users who have seen many movies and the movies that have been seen by many users?
  
&nbsp;

To identify and select the most relevant users and movies, let's determine first:

&nbsp;

1.  Some movies are rated more often than others. On the plot we can see distribution of movies based on user ratings

&nbsp;

```{r nr-movie-user-plot, eval=TRUE,cache=TRUE}
edx %>%  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = 'black') + 
  scale_x_log10() + 
  ggtitle('Distribution of movies')
```

&nbsp;

2. Some users are more active than others at rating movies:

```{r nr-user-movie-plot, eval=TRUE,cache=TRUE}
edx %>%  count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = 'black') + 
  scale_x_log10() + 
  ggtitle('Distribution of users')
```

&nbsp;

## Building models

Following instruction provided in professor Rafael Irizarry course, I will start by building the simplest recommendation system: 


### Model 1 : Computing predicted ratings for all movies regardless of user

Our first model that assumes the same rating for all movies and users with all the differences explained by random variation would look like this:

$$
Y_{u,i} = \mu + \varepsilon_{u,i}
$$

&nbsp;

where $\varepsilon_{i,u}$ are the i.i.d errors centered at 0 and $\mu$ the **true** rating for all movies.

&nbsp;

```{r mean-rating, eval=TRUE,cache=TRUE}
# Rating for all movies
mu_hat <- mean(edx$rating)
mu_hat
```

&nbsp;

If we want to assess the strength of fit, one method is to check how far off the model is for a typical case. That is, for some observations, the fitted value will be very close to the actual value, while for others it will not. The magnitude of a typical residual can give us a sense of generally how close our estimates are.Least squares fitting procedure guarantee that the mean of the residuals is zero. Thus, it makes more sense to compute **root mean squared error (RMSE)**. We will use function **RMSE** from Prof. Rafael A. Irizarry lectures.


```{r rmse-function, eval=TRUE,cache=TRUE}
#RMSE function
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```


&nbsp;

```{r simple-model-rmse, eval=TRUE,cache=TRUE}
# RMSE calculation
simple_model_rmse <- RMSE(validation$rating, mu_hat) 
simple_model_rmse
```

&nbsp;

We see that our first evaluation for RMSE is `r simple_model_rmse` a little bit higher that 1. We will continue comparing different approaches to check if we can get a lower value for RMSE. 

&nbsp;

Below we create a results table to store all RMSE values we get in different approaches:
&nbsp;

```{r rmse-table, eval=TRUE,cache=TRUE}
rmse_values <- tibble(method = 'Simple model RMSE', RMSE = simple_model_rmse)
knitr::kable(rmse_values)
```


### Model 2 : Computing predicted ratings for all movies based on movie effects


During data exploration, we noticed that some movies are just generally rated higher than others. 
We will add in our previously built simple model the term $b_i$ to represent average ranking for movie $i$: 

$$
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
$$

We will refer to the $b$s as  **effects** or **bias**, movie-specific effect. We will estimate $b_i$-s using **least square method**. Function **lm** makes this possible, but it can be very slow so we will procced by taking Professor Rafael Irizarry's advice.

In this particular situation, we know that the least squares estimate $\hat{b}_i$ is just the average of $Y_{u,i} - \hat{\mu}$ for each movie $i$. So we can compute them this way:
  
  &nbsp;

```{r movie-effect, eval=TRUE,cache=TRUE}
#Compute the average of all ratings of the edx set
mu <- mean(edx$rating)
#Compute b_i
movie_avgs <- edx %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))
```

&nbsp;

Let's plot now the estimated $b_i$-s distribution:

&nbsp;

```{r bi-plot, eval=TRUE,cache=TRUE}
#Plot b_i distribution
movie_avgs %>% 
ggplot(aes(b_i)) + 
geom_histogram(bins = 30, color = 'black') + 
ggtitle('Distribution of estimated b_i')
```

&nbsp;

From the plot we can see that these estimates vary substantially.

&nbsp;

```{r model2-pred, eval=TRUE,cache=TRUE}
# Predict bi
model_2_pred <- mu + validation %>% 
left_join(movie_avgs, by='movieId') %>%
.$b_i
movie_effect_rmse <- RMSE(model_2_pred, validation$rating)
# Enter RMSE value in table 
rmse_values <- bind_rows(rmse_values,
                          tibble(method='Movie Effect Model',  
                                 RMSE = movie_effect_rmse))
knitr::kable(rmse_values)
```



Our model has improved with movie effect added. Let's try to get it better.

### Model 3 : Computing predicted ratings for all movies based on movie and user effects

Another improvement to our model may be:
  
  &nbsp;

$$ 
  Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
$$
  
  where $b_u$ is a user-specific effect. Why should we think that adding user effect can lead to model improvement? Let’s compute the average rating for user $u$ for those that have rated over 100 movies and plot the estimated $b_u$-s distribution:
  
  &nbsp;

```{r avg-rate-100, eval=TRUE,cache=TRUE}
# Compute average rating for user u who rated more than 100 movies
edx %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = 'black') + 
  ggtitle('Distribution of estimated b_u')
```

&nbsp;

We notice that there is substantial variability across users as well so we proceed with model fitting. Let's compute an approximation by computing $\hat{\mu}$ and $\hat{b}_i$ and estimating $\hat{b}_u$ as the average of $y_{u,i} - \hat{\mu} - \hat{b}_i$:

&nbsp;

```{r movie-user, eval=TRUE,cache=TRUE}
#Compute b_u on edx 
user_avgs <- edx %>%  
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))
```

&nbsp;

Now let's see if RMSE improves:
  
  &nbsp;

```{r model3-rmse, eval=TRUE,cache=TRUE}
# Predicted ratings
model3_pred <- validation %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)
movie_user_effect <- RMSE(model3_pred, validation$rating)
rmse_values <- bind_rows(rmse_values,
                         tibble(method='Movie + User Effects Model',  
                                RMSE = movie_user_effect))
knitr::kable(rmse_values)
```




Our RMSE is lower now but still not at the target we are looking for.

## Regularization

For any machine learning problem, essentially, we can divide data points into two components — pattern + stochastic noise. The goal of machine learning is to model the pattern and ignore the noise. Probably our algorithm is trying to fit the noise in addition to the pattern, so we are dealing with **overfitting**.


Maybe we can hold back and check if our algorithm is fitting the noise. Let's explore problems in our first model, using only movie effects $b_i$  following Prof. Rafael Irizarry approach.

```{r model-problems, eval=TRUE,cache=TRUE}
validation %>%
left_join(movie_avgs, by='movieId') %>%
mutate(residual = rating - (mu + b_i)) %>%
arrange(desc(abs(residual))) %>%
select(title, residual) %>% slice(1:10)
```

We notice some large predictions for many of movies in the above table. Let’s look at the top 10 worst and best movies based on $b_i$. First, let’s create a database that connects **movieId** to movie **title**:

```{r merge-db, eval=TRUE,cache=TRUE}
# merged database
merge_db <- edx %>% 
select(movieId, title) %>%
distinct()
```

Here are the 10 best movies according to our estimate and how often they were rated:

```{r best-10-bi, eval=TRUE,cache=TRUE}
# top 10 best movies based on b_i
movie_avgs %>% left_join(merge_db, by="movieId") %>%
arrange(desc(b_i)) %>%
select(title, b_i) %>%
slice(1:10) 
validation %>% count(movieId) %>%
left_join(movie_avgs) %>%
left_join(merge_db, by="movieId") %>%
arrange(desc(b_i)) %>%
select(title, b_i, n) %>%
slice(1:10)
```


And here are the 10 worst. Let's look at how often they are rated.

```{r worst-10-bi, eval=TRUE,cache=TRUE}
# top 10 worse movies based on b_i
movie_avgs %>% left_join(merge_db, by="movieId") %>%
arrange(b_i) %>%
select(title, b_i) %>%
slice(1:10)
knitr::kable(movie_avgs)
validation %>% count(movieId) %>%
left_join(movie_avgs) %>%
left_join(merge_db, by="movieId") %>%
arrange(b_i) %>%
select(title, b_i, n) %>%
slice(1:10)
```



### Penalized least squares

The general idea behind regularization is to constrain the **total variability** of the effect sizes. We make this possible by adding a **penalty term** to the least square equation. This **penalty term** gets larger when $b_i$ are large. So we consider **penalty term** a parameter which we have to tune to get optimal results.


We will use $\lambda$ is a tuning parameter. We can use cross-validation to choose it.

```{r lambda-cross, eval=TRUE,cache=TRUE}
lambdas <- seq(0, 10, 0.25)
mu <- mean(edx$rating)
just_the_sum <- edx %>% 
  group_by(movieId) %>% 
  summarize(s = sum(rating - mu), n_i = n())
rmses <- sapply(lambdas, function(l){
  predicted_ratings <- validation %>% 
    left_join(just_the_sum, by='movieId') %>% 
    mutate(b_i = s/(n_i+l)) %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  return(RMSE(predicted_ratings, validation$rating))
})
```

Now we plot RMSE values together with lambdas:

```{r lambda-plot, eval=TRUE,cache=TRUE}
# Plot lambdas and rmse
ggplot(data.frame(lambdas = lambdas, rmses = rmses ), aes(lambdas, rmses)) +
  geom_point()
lambdas[which.min(rmses)]
```


We can use regularization for the estimate user effects as well. We are minimizing:
  
$$
  \frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i - b_u \right)^2 + 
  \lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2\right)
$$
  
  The estimates that minimize this can be found similarly to what we did above. Here we use cross-validation to pick a $\lambda$:
  
```{r best-lambda, eval=TRUE}
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
mu <- mean(edx$rating)

b_i <- edx %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
b_u <- edx %>% 
    left_join(b_i, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
    predicted_ratings <- 
    validation %>% 
    left_join(b_i, by = 'movieId') %>%
    left_join(b_u, by = 'userId') %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
return(RMSE(predicted_ratings, validation$rating))
})
ggplot(data.frame(lambdas = lambdas, rmses = rmses ), aes(lambdas, rmses)) +
geom_point()  
```

For the full model, the optimal $\lambda$ is:
  
```{r lambda,cache=TRUE}
# Value of lambda that minimizes  RMSE
lambda <- lambdas[which.min(rmses)]
lambda
```


```{r rmue-rmse-values, echo=FALSE,cache=TRUE}
# Add model with the minimal RMSE to the results data frame
rmse_values <- bind_rows(
  rmse_values,
  tibble(method='Regularized Movie + User Effect Model',  
         RMSE = min(rmses)))
knitr::kable(rmse_values)
```


## Results and conclusions

Finally we print again table of RMSE values for all models we build during this work:

```{r final-table-rmse, echo = FALSE}
knitr::kable(rmse_values)
```

We can see that lowest value RMSE we could achive so far is which is lower than our starting goal (0.8775). **movieId** variable has a large impact on the **rmse** value. When we combined this impact with **usedId** effect,  **rmse** value became smaller.

So our final model will be:

$$Y_{u, i} = \mu + b_{i} + b_{u} + \epsilon_{u, i}$$

